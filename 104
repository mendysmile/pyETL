import requests
from bs4 import BeautifulSoup
import json
from my_fake_useragent import UserAgent
from urllib import request
import os

def random_header():
    ua = UserAgent()
    random_header = json.loads(r'''{
    "Cache-Control": "max-age=0",
    "Connection": "keep-alive",
    "Host": "www.dogforum.com",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent":"%s"
    }'''%ua.random)
    return random_header
########################################################
"""
resource_path = r'./jobsearch'
if not os.path.exists(resource_path):
    os.mkdir(resource_path)
"""

headers = {'User-Agent':str(random_header())}

url = 'https://www.104.com.tw/jobs/search/?ro=0&kwop=7&keyword=%s&order=1&asc=0&page=%s&mode=s&jobsource=2018indexpoc'%('python',1)
req = requests.get(url,headers=headers)
soup = BeautifulSoup(req.text,'html.parser')
title = soup.select('a[class="js-job-link"]')

for i in title:
    print(i.text)
    print('https:' + i['href'])

    job_html = 'https:' + i['href']
    job_req = requests.get(job_html,headers=headers)
    job_soup = BeautifulSoup(job_req.text, 'html.parser')
    job_content = job_soup.select('section[class="info"]')
    #for j in job_content:
